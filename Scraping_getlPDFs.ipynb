{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCRAPING - ss 21 \n",
    "--------------- \n",
    "> Chiara Piccolroaz\n",
    "\n",
    "> University of Konstanz\n",
    "\n",
    "> Department of Computer and Information Science; Department of political and administration science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING PACKAGES\n",
    "--------------------------\n",
    "\n",
    "WEbseiten for Scraping Juris : https://www.juris.de/jportal/portal/ and https://rzblx10.uni-regensburg.de/dbinfo/detail.php?bib_id=ubko&colors=&ocolors=&lett=f&tid=0&titel_id=865\n",
    "\n",
    "WebSeite for Installing Selenium: : https://www.jcchouinard.com/learn-selenium-python-seo-automation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import selenium.webdriver.support.ui as ui\n",
    "import requests, PyPDF2\n",
    "from io import BytesIO\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEFOR STARTING:\n",
    "\n",
    " Make sure to: (1) be locked in the Uni-WLAN or connected with the Uni-VPN. (2) the packages have been download\n",
    "\n",
    "IDEE OF THE PROJEKT: \n",
    "\n",
    "It is for us impossible to download the PDF, which we need (is there maybe another way?). For this reason, we first extract the PDFs-links (Part1), so that we can use another function (Part2), to reopen the PDFs, extract the text and save it in a text-file. \n",
    "\n",
    "PROBLEME: \n",
    "\n",
    "(1) Ich werde nach ein paar Stunden Scraping automatisch von der Website abgemeldet, was dazu führt, dass der ganze Prozess aufhört und die Webseite nich nicht merken kann, welche die letzte Operation war. \n",
    "(2) Part 2 works only on not-Juris PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1. Get Links\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driverC=webdriver.Chrome(executable_path=r\"C:\\chromedriver_win32\\chromedriver.exe\") \n",
    "driverC.get('https://www.juris.de/jportal/?action=JLoginUser&username=HEBISUKN.autologin')\n",
    "driverC.get('https://www.juris.de/r3/search')\n",
    "driverC.maximize_window()\n",
    "pickle.dump(driverC.get_cookies() , open(\"QuoraCookies.pkl\",\"wb\"))\n",
    "\n",
    "for cookie in pickle.load(open(\"QuoraCookies.pkl\", \"rb\")): \n",
    "    driverC.add_cookie(cookie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the relevant filers and buttons. Make sure to: run the sleep functions, otherwise the other functions will override. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driverC.find_element_by_id(\"categories_hits_Rechtsprechung\")\n",
    "button.click()\n",
    "time.sleep(2) \n",
    "button = driverC.find_element_by_id(\"filter_Sachgebiete\")\n",
    "button.click()\n",
    "time.sleep(2) \n",
    "button = driverC.find_element_by_id(\"filter_Asylis___MILo\")\n",
    "button.click()\n",
    "time.sleep(2) \n",
    "button = driverC.find_element_by_id(\"filter_apply\")\n",
    "button.click()\n",
    "time.sleep(2) \n",
    "button = driverC.find_element_by_id(\"linkWorkId80896\")\n",
    "button.click()\n",
    "time.sleep(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the links: \n",
    "Collect the link of the pdfs for each of the 19151 elements. In order to do that use the following function, which enters each of the 19151 element files, get all PDFs-links and save them in a list. \n",
    "\n",
    "We now try for 10 PDFs. The number 10 must be replaced by 19151 when running the projekt for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultlistentry_LI0\n",
      "no PDF\n",
      "resultlistentry_LI1\n",
      "no PDF\n",
      "resultlistentry_LI2\n",
      "no PDF\n",
      "resultlistentry_LI3\n",
      "got the PDFs, go one back\n",
      "resultlistentry_LI4\n",
      "got the PDFs, go one back\n",
      "resultlistentry_LI5\n",
      "got the PDFs, go one back\n",
      "resultlistentry_LI6\n",
      "got the PDFs, go one back\n",
      "resultlistentry_LI7\n",
      "no PDF\n",
      "resultlistentry_LI8\n",
      "got the PDFs, go one back\n",
      "resultlistentry_LI9\n",
      "got the PDFs, go one back\n"
     ]
    }
   ],
   "source": [
    "# 0. Initialisation\n",
    "\n",
    "# the list the PDF-files are gonna be saved in\n",
    "listPDFs = [] \n",
    "# i refers to the number of ducuments pro page and gives the function the id of the document we are looking for. Put i = j = 0\n",
    "i = 0 \n",
    "# j refers to the number of documents in total. Put i = j = 0\n",
    "j = 0\n",
    "# p refers to the number of the page \n",
    "p = 1\n",
    "\n",
    "# 0. repeat the function for the number of the documents: \n",
    "while j < 10:\n",
    "    # get the name\n",
    "    # create the string, which indicates the element_id on the website, which we want to enter the element file with \n",
    "    str1 ='resultlistentry_LI' \n",
    "    str2 = \"{}\".format(i)\n",
    "    str = str1 + str2\n",
    "    print(str) \n",
    "    \n",
    "# 1. enter the element´s file\n",
    "    Urteil = driverC.find_element_by_id(str) \n",
    "    Urteil.click()\n",
    "    time.sleep(1) \n",
    "    \n",
    "# 2. get the PDF-link(s) from the respective element´s file\n",
    "    # get the links and put in the list linkList\n",
    "    links=driverC.find_elements_by_tag_name(\"a\")\n",
    "    linkList = [link.get_attribute('href') for link in links]\n",
    "    time.sleep(1)\n",
    "    # clean the list linkList, in order to have only strings\n",
    "    clean_linkList = [i for i in linkList if i is not None]\n",
    "    # count how many PDFs are, which we are interested in. \n",
    "    # According to that we have 3 different functions to save the links and the texts of the PDF \n",
    "    count = 0\n",
    "    sub = '/jportal/docs/anlage/as/bilder/'\n",
    "    for link in clean_linkList:\n",
    "        if sub in link:\n",
    "            count = count +1\n",
    "    # if these are more than just one PDF: \n",
    "    ### select the links, save them as list linksPDF and add it to the list linkPDF\n",
    "    linkPDF = []\n",
    "    TextList = []\n",
    "    if count > 1:\n",
    "        print('multiple PDFs')\n",
    "        for link in clean_linkList:\n",
    "            if sub in link:\n",
    "                linkPDF.append(link)      \n",
    "    \n",
    "    #if there is only one PDF\n",
    "    ### select the link and add to the list linkPDF;       \n",
    "    if count == 1:\n",
    "        print(\"got the PDFs, go one back\")\n",
    "        for link in clean_linkList:\n",
    "            if sub in link:\n",
    "                linkPDF = link\n",
    "    \n",
    "    #if there are no PDFs: do nothing  \n",
    "    if count < 1:\n",
    "        print('no PDF') \n",
    "        \n",
    "# 4. add the PDF(s) in the ListPDFs\n",
    "    listPDFs.append(linkPDF)\n",
    "    #print(listPDFs)\n",
    "    time.sleep(1)\n",
    "    \n",
    "# 5. go back to the main page and go on with the other elements\n",
    "    backtoList = driverC.find_element_by_class_name(\"tlrahmen\")\n",
    "    backtoList.click()\n",
    "    time.sleep(3) \n",
    "    \n",
    "# 6. increase i to check for the elements pro page; increase j to check for the 19151 elements\n",
    "    i = i+1\n",
    "    j = j+1 \n",
    "    \n",
    "# 6. if te last element change the page \n",
    "    if i == 25: \n",
    "        print(\"i is equal 25 - we got the 25th element of the page -, so change page\")  \n",
    "        p = p+1\n",
    "        print(\"NEXT page\" , p)\n",
    "        driverC.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        button = driverC.find_element_by_class_name(\"svg-icon-chevron_right\")\n",
    "        button.click()\n",
    "        time.sleep(3)  \n",
    "        i= 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function we want to restructure the list of links, by modifing the sublists: that means eleminating the empty  sublistst and ectracting the links from the sublists which containingmultiple informations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eleminating the empty  sublistst\n",
    "listPDFs_cleaned1 = [x for x in listPDFs if x]\n",
    "\n",
    "# exctracting the links from the sublists which connatins multiple informations\n",
    "def flatten(A):\n",
    "    rt = []\n",
    "    for i in A:\n",
    "        if isinstance(i,list): rt.extend(flatten(i))\n",
    "        else: rt.append(i)\n",
    "    return rt\n",
    "\n",
    "listPDFs_cleaned2 = flatten(listPDFs_cleaned1)\n",
    "listPDFs_cleaned = listPDFs_cleaned2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.juris.de/jportal/docs/anlage/as/bilder/bafl/18455390.pdf',\n",
       " 'https://www.juris.de/jportal/docs/anlage/as/bilder/bafl/17130144.pdf',\n",
       " 'https://www.juris.de/jportal/docs/anlage/as/bilder/bafl/16634543.pdf',\n",
       " 'https://www.juris.de/jportal/docs/anlage/as/bilder/bafl/16387029.pdf',\n",
       " 'https://www.juris.de/jportal/docs/anlage/as/bilder/bafl/15207340.pdf',\n",
       " 'https://www.juris.de/jportal/docs/anlage/as/bilder/bafl/19439387.pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listPDFs_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the links into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Link.txt', 'w') as f:\n",
    "    for item in listPDFs_cleaned:\n",
    "        f.write(\"%s\\r\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: Get PDFs\n",
    "--------------------------\n",
    "This function can extract the text of PDFs which have been saved in a list, in our case in the list listPDFs_cleaned. Sadly in does not work for Juris-PDFs, but only for normal PDFs (You can run an example, with another link to a pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PdfReadError",
     "evalue": "Cannot read an empty file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPdfReadError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-94497efc0a98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_raw_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mread_pdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_pdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stream, strict, warndest, overwriteWarnings)\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\pdf.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1691\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot read an empty file'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1692\u001b[0m         \u001b[0mlast1K\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1024\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# offset of last 1024 bytes of stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPdfReadError\u001b[0m: Cannot read an empty file"
     ]
    }
   ],
   "source": [
    "# Diese Funktion fuktionier nur mit nicht-Juris-PDFs\n",
    "TextList = []\n",
    "\n",
    "for link in listPDFs_cleaned:\n",
    "    Pages = []\n",
    "    response = requests.get(link)\n",
    "    my_raw_data = response.content\n",
    "    \n",
    "    with BytesIO(my_raw_data) as data:\n",
    "        read_pdf = PyPDF2.PdfFileReader(data)\n",
    "        \n",
    "        for page in range(read_pdf.getNumPages()):\n",
    "            p =read_pdf.getPage(page).extractText()\n",
    "            p = p.replace(\"\\n\", \"\")\n",
    "            Pages.append(p)\n",
    "        JointPages = \"\".join(Pages)\n",
    "        TextList.append(JointPages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the texts of the PSDFs into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as f:\n",
    "    for item in TextList:\n",
    "        f.write(\"%s\\r\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
